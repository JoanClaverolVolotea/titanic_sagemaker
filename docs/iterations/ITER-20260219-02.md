# ITER-20260219-02

## Objetivo y contexto
Completar los pasos 1 al 4 de `docs/tutorials/02-training-validation.md` para dejar listos los insumos de entrenamiento XGBoost en S3 antes de crear el Training Job en SageMaker.

## Decisiones tecnicas y alternativas descartadas
- Se ejecuto el flujo exactamente con perfil `data-science-user` y region `eu-west-1`.
- Se obtuvo el bucket de trabajo desde `terraform/00_foundations` (`output -raw data_bucket_name`) para evitar hardcode.
- Se mantuvo el formato esperado por SageMaker Built-in XGBoost: CSV sin header.
- Alternativa descartada: saltar validacion de `curated/train.csv` y `curated/validation.csv` antes de preparar/subir artefactos.

## IAM usado (roles/policies/permisos clave)
- Identidad humana operativa: `arn:aws:iam::939122281183:user/data-science-user`.
- Perfil AWS CLI: `data-science-user`.
- Permisos usados en esta iteracion:
  - `s3:ListBucket`
  - `s3:GetObject`
  - `s3:PutObject`
  - lectura de output Terraform para resolver bucket de fase 00.

## Comandos ejecutados y resultado esperado
1. Definicion de variables:
   - `AWS_PROFILE=data-science-user`
   - `AWS_REGION=eu-west-1`
   - `DATA_BUCKET=$(terraform -chdir=terraform/00_foundations output -raw data_bucket_name)`
2. Validacion de datos de fase 01 en S3:
   - `aws s3 ls s3://$DATA_BUCKET/curated/train.csv --profile data-science-user`
   - `aws s3 ls s3://$DATA_BUCKET/curated/validation.csv --profile data-science-user`
3. Preparacion local de inputs XGBoost:
   - `python3 scripts/prepare_titanic_xgboost_inputs.py`
   - `wc -l data/titanic/sagemaker/train_xgb.csv data/titanic/sagemaker/validation_xgb.csv data/titanic/sagemaker/validation_features_xgb.csv data/titanic/sagemaker/validation_labels.csv`
4. Upload a S3:
   - `aws s3 cp data/titanic/sagemaker/train_xgb.csv s3://$DATA_BUCKET/training/xgboost/train_xgb.csv --profile data-science-user`
   - `aws s3 cp data/titanic/sagemaker/validation_xgb.csv s3://$DATA_BUCKET/training/xgboost/validation_xgb.csv --profile data-science-user`
   - `aws s3 cp data/titanic/sagemaker/validation_features_xgb.csv s3://$DATA_BUCKET/training/xgboost/validation_features_xgb.csv --profile data-science-user`
   - `aws s3 cp data/titanic/sagemaker/validation_labels.csv s3://$DATA_BUCKET/training/xgboost/validation_labels.csv --profile data-science-user`
5. Verificacion final:
   - `aws s3 ls s3://$DATA_BUCKET/training/xgboost/ --profile data-science-user`

Resultado esperado cumplido: los 4 artefactos quedaron disponibles en S3 para la fase de entrenamiento.

## Evidencia (outputs, logs, metricas)
- Bucket resuelto: `titanic-data-bucket-939122281183-data-science-user`.
- Objetos `curated/*` encontrados:
  - `train.csv` (48,577 bytes)
  - `validation.csv` (11,805 bytes)
- Script de preparacion:
  - `Prepared XGBoost inputs: train=713 validation=178 age_fill=28.2500 fare_fill=15.2458`
- Conteos locales:
  - `train_xgb.csv`: 713 lineas
  - `validation_xgb.csv`: 178 lineas
  - `validation_features_xgb.csv`: 178 lineas
  - `validation_labels.csv`: 178 lineas
- Verificacion final en S3 (`training/xgboost/`):
  - `train_xgb.csv` (23,726 bytes)
  - `validation_xgb.csv` (5,904 bytes)
  - `validation_features_xgb.csv` (5,548 bytes)
  - `validation_labels.csv` (356 bytes)

## Riesgos/pendientes
- Pendiente paso 5+ del tutorial: crear Training Job, Model y Batch Transform.
- Si cambian columnas o preprocesamiento local sin documentar, puede aparecer drift frente al pipeline de fase 03.
- Mantener limpieza de artefactos temporales en `data/titanic/sagemaker/` entre iteraciones para evitar confusion operativa.

## Proximo paso
Ejecutar el paso 5 de `docs/tutorials/02-training-validation.md`: crear `Training Job` en SageMaker (algoritmo Built-in XGBoost) usando los URIs `s3://$DATA_BUCKET/training/xgboost/train_xgb.csv` y `.../validation_xgb.csv`.
